{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXBHFBHriBp4"
      },
      "source": [
        "# Mounting Google Drive and getting missing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QIyzxBo0oun",
        "outputId": "77f8e543-66b5-4d6a-fd4a-f38bdcc62833"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  os.chdir(\"drive/MyDrive/ViT_Lung_Cancer-main\")\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6XxMc351c10",
        "outputId": "58742b73-4185-4f81-ba24-04420b1ff05c"
      },
      "outputs": [],
      "source": [
        "# %pip install ml_collections\n",
        "# %pip install einops\n",
        "# %pip install monai\n",
        "import monai\n",
        "# monai.config.print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efIMZl9PAtEX"
      },
      "source": [
        "# Parts of Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YwaZxT-QBjzH"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "#from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.transforms import Resize\n",
        "from torch.nn import CrossEntropyLoss, MSELoss, Dropout, Softmax, Linear, Conv2d, LayerNorm, Conv3d, AdaptiveAvgPool3d, MultiLabelSoftMarginLoss, BCELoss, Sigmoid, Conv1d, BCEWithLogitsLoss\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "import models.configs as configs\n",
        "\n",
        "# from models.modeling_resnet import ResNetV2\n",
        "# from models.coatnet import CoAtNet\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ka_mXlqqAgp9"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor, in_channels = in_channels)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "    \n",
        "\n",
        "class Embeddings3D(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3, depth_kernel_size = 5, out_depth = 3):\n",
        "        super(Embeddings3D, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "        self.depth_kernel_size = depth_kernel_size\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])* out_depth\n",
        "            self.hybrid = False\n",
        "\n",
        "        self.patch_embeddings = Conv3d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=(depth_kernel_size,patch_size[0], patch_size[1]),\n",
        "                                       stride=(depth_kernel_size,patch_size[0], patch_size[1]))\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "        \n",
        "        self.adapt_pool = AdaptiveAvgPool3d((out_depth, None, None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        #x  is [B, C, H, W, D]\n",
        "        x = x.permute(0,1,4,2,3) #x = [B, C, D, H, W] for Conv3D\n",
        " \n",
        "        #replicate the last slice to obtain a number of slice divisible by self.depth_kernel_size \n",
        "        num_rep = int(math.ceil(x.shape[2]/self.depth_kernel_size)*self.depth_kernel_size)\n",
        "        for r in range(x.shape[2], num_rep):\n",
        "            x = torch.cat((x, torch.zeros([x.shape[0], x.shape[1], 1, x.shape[3], x.shape[4]]).to(x.device)), 2)\n",
        "            x = x.float()\n",
        "        x = self.patch_embeddings(x)\n",
        "        #apply adaptive average pooling to have the same depth for each CT\n",
        "        x = self.adapt_pool(x) # [B, C, D, H, W]\n",
        "        #permute to [B, C, H, W, D]\n",
        "        x = x.permute(0,1,3,4,2) #x = [B, C, H, W, D]\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.attn = Attention(config, vis)\n",
        "        self.ffn = Mlp(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            #query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            query_weight = np2th(weights[ROOT +'/'+ ATTENTION_Q + '/' +\"kernel\"]).view(self.hidden_size, self.hidden_size).t()\n",
        "            #key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[ROOT + '/' + ATTENTION_K+ '/' +\"kernel\"]).view(self.hidden_size, self.hidden_size).t()\n",
        "            #value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[ROOT + '/' + ATTENTION_V + '/' +\"kernel\"]).view(self.hidden_size, self.hidden_size).t()\n",
        "            #out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[ROOT+ '/' + ATTENTION_OUT+ '/' +\"kernel\"]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            #query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            query_bias = np2th(weights[ROOT + '/' + ATTENTION_Q + '/' + \"bias\"]).view(-1)\n",
        "            #key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[ROOT+'/'+ ATTENTION_K +'/'+ \"bias\"]).view(-1)\n",
        "            #value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[ROOT +'/'+ ATTENTION_V +'/'+ \"bias\"]).view(-1)\n",
        "            #out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[ROOT +'/'+ ATTENTION_OUT +'/'+ \"bias\"]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            #mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_0 = np2th(weights[ROOT+'/'+ FC_0 +'/'+ \"kernel\"]).t()\n",
        "            #mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[ROOT+'/'+ FC_1+'/' + \"kernel\"]).t()\n",
        "            #mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[ROOT +'/'+ FC_0 +'/'+ \"bias\"]).t()\n",
        "            #mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[ROOT +'/'+ FC_1 +'/'+ \"bias\"]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            #self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.weight.copy_(np2th(weights[ROOT +'/'+ ATTENTION_NORM +'/'+ \"scale\"]))\n",
        "            #self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[ROOT+'/'+ ATTENTION_NORM+'/'+ \"bias\"]))\n",
        "            #self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[ROOT+'/'+ MLP_NORM+'/'+ \"scale\"]))\n",
        "            #self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[ROOT+'/'+ MLP_NORM+'/'+ \"bias\"]))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):                                                                                       # Embedding + encoder\n",
        "    def __init__(self, config, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth):\n",
        "        super(Transformer, self).__init__()\n",
        "        if embeddings_type == '2D':\n",
        "            self.embeddings = Embeddings(config, img_size=img_size, in_channels=in_channels)\n",
        "        elif embeddings_type == '3D':\n",
        "            self.embeddings = Embeddings3D(config, img_size=img_size, in_channels=in_channels, depth_kernel_size = depth_kernel_size,  out_depth = out_depth)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module): # Transformer + loss function\n",
        "    def __init__(self, config, img_size=224, in_channels=3, num_classes=21843, loss_weights=None, zero_head=False, vis=False, embeddings_type = '2D', depth_kernel_size = 5, out_depth = 3, loss_type = 'CrossEntropy'):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.loss_type = loss_type\n",
        "        self.classifier = config.classifier\n",
        "        self.in_channels = in_channels\n",
        "        self.transformer = Transformer(config, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth)\n",
        "        \n",
        "        self.sigmoid = Sigmoid()\n",
        "        \n",
        "        self.head = Linear(config.hidden_size, self.num_classes)                                                       # Classification layer\n",
        "        \n",
        "           \n",
        "        if 'grid' not in config.patches.keys():\n",
        "            self.patch_size = config.patches.size\n",
        "        else:\n",
        "            self.patch_size = None\n",
        "        self.loss_weights = loss_weights # useful when you have an unbalanced training set\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "        \n",
        "        if labels is not None:\n",
        "            if self.loss_type == 'CrossEntropy':\n",
        "                loss_fct = CrossEntropyLoss(weight = self.loss_weights.to(x.device) if torch.is_tensor(self.loss_weights) else None)\n",
        "            elif self.loss_type== 'MSE':\n",
        "                loss_fct = MSELoss()\n",
        "            elif self.loss_type == 'MultiLabelSoftMarginLoss':\n",
        "                preds = self.sigmoid(logits)\n",
        "                loss_fct = MultiLabelSoftMarginLoss()\n",
        "            elif self.loss_type == 'BCELoss':\n",
        "                preds = self.sigmoid(logits)\n",
        "                loss_fct = BCELoss()\n",
        "            elif self.loss_type == 'BCEWithLogits':\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "            else:\n",
        "                raise NameError('ATTENTION! Loss type not managed')\n",
        "                \n",
        "            if self.loss_type in ['MultiLabelSoftMarginLoss']:\n",
        "                loss = loss_fct(preds, labels)\n",
        "            elif self.loss_type in ['MSE', 'BCEWithLogits']:\n",
        "                loss = loss_fct(logits.float(), labels.float())\n",
        "            elif self.loss_type == 'BCELoss':\n",
        "                loss = loss_fct(preds, labels.float())\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_classes), labels)\n",
        "            return logits, loss\n",
        "        \n",
        "        return logits, attn_weights\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "                #else:\n",
        "                    #nn.init.zeros_(self.head[0].weight)\n",
        "                    #nn.init.zeros_(self.head[0].bias)\n",
        "                    #nn.init.zeros_(self.head[2].bias)\n",
        "                    #nn.init.zeros_(self.head[2].weight)\n",
        "                  \n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "                \n",
        "            new_weights = np2th(weights[\"embedding/kernel\"], conv=True)\n",
        "            if self.in_channels == 1 :\n",
        "                new_weights = new_weights.mean(dim = 1, keepdim = True)\n",
        "            elif self.in_channels == 2:\n",
        "                new_weights = new_weights.mean(dim = 1, keepdim = True)\n",
        "                new_weights = torch.cat([new_weights, new_weights], dim = 1)\n",
        "                \n",
        "            if self.patch_size is not None and self.patch_size != (16,16):\n",
        "                scale_factor = self.patch_size[0] / 16\n",
        "                new_weights = torch.nn.functional.interpolate(new_weights,scale_factor = scale_factor, mode = 'bilinear')\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(new_weights)\n",
        "            \n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
        "                        \n",
        "\n",
        "class ParallelVisionTransformer(nn.Module): # Parallel Transformer + loss function\n",
        "    def __init__(self, config, img_size=224, in_channels=3, num_classes=21843, loss_weights=None, zero_head=False, vis=False, embeddings_type = '2D', depth_kernel_size = 5, out_depth = 3, loss_type = 'CrossEntropy'):\n",
        "        super(ParallelVisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.loss_type = loss_type\n",
        "        self.classifier = config.classifier\n",
        "        self.in_channels = in_channels\n",
        "        \n",
        "        assert img_size%16==0, 'Image size must be divisible by 16'\n",
        "        patch_size = int(img_size/2)\n",
        "        config1 = copy.deepcopy(config)\n",
        "        config1.patches.size =(patch_size, patch_size)\n",
        "        patch_size = int(patch_size/2)\n",
        "        config2 = copy.deepcopy(config)\n",
        "        config2.patches.size =(patch_size, patch_size)\n",
        "        patch_size = int(patch_size/2)\n",
        "        config3 = copy.deepcopy(config)\n",
        "        config3.patches.size =(patch_size, patch_size)\n",
        "        patch_size = int(patch_size/2)\n",
        "        config4 = copy.deepcopy(config)\n",
        "        config4.patches.size =(patch_size, patch_size)\n",
        "        \n",
        "        self.transformer1 = Transformer(config1, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth)\n",
        "        self.transformer2 = Transformer(config2, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth)\n",
        "        self.transformer3 = Transformer(config3, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth)\n",
        "        self.transformer4 = Transformer(config4, img_size, in_channels, vis, embeddings_type, depth_kernel_size, out_depth)\n",
        "        \n",
        "        self.sigmoid = Sigmoid()\n",
        "        \n",
        "        self.last_conv = Conv1d(4, 1, kernel_size = 1)\n",
        "        \n",
        "        self.head = Linear(config.hidden_size, self.num_classes)\n",
        "        \n",
        "           \n",
        "        if 'grid' not in config.patches.keys():\n",
        "            self.patch_size = config.patches.size\n",
        "        else:\n",
        "            self.patch_size = None\n",
        "        self.loss_weights = loss_weights\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x1, attn_weights1 = self.transformer1(x)\n",
        "        x2, attn_weights2 = self.transformer2(x)\n",
        "        x3, attn_weights3 = self.transformer3(x)\n",
        "        x4, attn_weights4 = self.transformer4(x)\n",
        "        \n",
        "        #print('x1 ', x1.shape)\n",
        "        #print('x2 ', x2.shape)\n",
        "        #print('x3 ', x3.shape)\n",
        "        #print('x4 ', x4.shape)\n",
        "        x = torch.stack([x1[:,0,:],x2[:,0,:],x3[:,0,:],x4[:,0,:]], dim = 1)\n",
        "        #print('stack ', x.shape)\n",
        "        x = self.last_conv(x)\n",
        "        #print('conv ', x.shape)\n",
        "        \n",
        "        logits = self.head(x[:, 0])\n",
        "        #print('logits ', logits.shape)\n",
        "        \n",
        "        if labels is not None:\n",
        "            if self.loss_type == 'CrossEntropy':\n",
        "                loss_fct = CrossEntropyLoss(weight = self.loss_weights.to(x.device) if torch.is_tensor(self.loss_weights) else None)\n",
        "            elif self.loss_type== 'MSE':\n",
        "                loss_fct = MSELoss()\n",
        "            elif self.loss_type == 'MultiLabelSoftMarginLoss':\n",
        "                preds = self.sigmoid(logits)\n",
        "                loss_fct = MultiLabelSoftMarginLoss()\n",
        "            elif self.loss_type == 'BCELoss':\n",
        "                preds = self.sigmoid(logits)\n",
        "                loss_fct = BCELoss()\n",
        "            else:\n",
        "                raise NameError('ATTENTION! Loss type not managed')\n",
        "                \n",
        "            if self.loss_type in ['MultiLabelSoftMarginLoss']:\n",
        "                loss = loss_fct(preds, labels)\n",
        "            elif self.loss_type in ['MSE']:\n",
        "                loss = loss_fct(logits.float(), labels.float())\n",
        "            elif self.loss_type == 'BCELoss':\n",
        "                loss = loss_fct(preds, labels.float())\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_classes), labels)\n",
        "            return logits, loss\n",
        "        \n",
        "        return logits, {'attn_weights1': attn_weights1, 'attn_weights2': attn_weights2, 'attn_weights3': attn_weights3, 'attn_weights4': attn_weights4}\n",
        "    \n",
        "    \n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "                #else:\n",
        "                    #nn.init.zeros_(self.head[0].weight)\n",
        "                    #nn.init.zeros_(self.head[0].bias)\n",
        "                    #nn.init.zeros_(self.head[2].bias)\n",
        "                    #nn.init.zeros_(self.head[2].weight)\n",
        "                  \n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "                \n",
        "            new_weights = np2th(weights[\"embedding/kernel\"], conv=True)\n",
        "            if self.in_channels == 1 :\n",
        "                new_weights = new_weights.mean(dim = 1, keepdim = True)\n",
        "            elif self.in_channels == 2:\n",
        "                new_weights = new_weights.mean(dim = 1, keepdim = True)\n",
        "                new_weights = torch.cat([new_weights, new_weights], dim = 1)\n",
        "                \n",
        "            if self.patch_size is not None and self.patch_size != (16,16):\n",
        "                scale_factor = self.patch_size[0] / 16\n",
        "                new_weights = torch.nn.functional.interpolate(new_weights,scale_factor = scale_factor, mode = 'bilinear')\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(new_weights)\n",
        "            \n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
        "\n",
        "def compute_att_map(attn_weights, device, imgs_size):\n",
        "    att_mat = torch.stack(attn_weights, dim = 1) # att_mat --> [B, num_layers, num_heads, num_patches+1, num_patches+1]\n",
        "    # Averages the attention weights across all heads\n",
        "    att_mat = torch.mean(att_mat, dim = 2) # att_mat --> [B, num_layers, num_patches+1, num_patches+1]\n",
        "    # To account for residual connections, we add an identity matrix to the\n",
        "    # attention matrix and re-normalize the weights.\n",
        "    residual_att = torch.eye(att_mat.size(2)).to(device) # Creating the identity matrix\n",
        "    aug_att_mat = att_mat + residual_att # sum\n",
        "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1) # [B, num_layers, num_patches+1, num_patches+1] # normalization\n",
        "    \n",
        "    # Recursively multiply the weight matrices to compute the attention rollout\n",
        "    joint_attentions = torch.zeros(aug_att_mat.size()).to(device)     # [B, num_layers, num_patches+1, num_patches+1] # empty matrix\n",
        "    for i in range(aug_att_mat.size(0)):\n",
        "        joint_attentions[i][0] = aug_att_mat[i][0]\n",
        "        \n",
        "        for n in range(1, aug_att_mat.size(1)):\n",
        "            joint_attentions[i][n] = torch.matmul(aug_att_mat[i][n], joint_attentions[i][n-1])\n",
        "            \n",
        "    # Attention of the last transformer\n",
        "    v = joint_attentions[:,-1] # [B, num_patches+1, num_patches+1]\n",
        "    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
        "    masks = v[:,0, 1:].reshape(-1, grid_size, grid_size).detach()\n",
        "    masks = (masks/masks.amax(dim = (1,2), keepdim = True))\n",
        "    \n",
        "    masks = Resize((imgs_size(1), imgs_size(2)))(masks)\n",
        "    \n",
        "    return masks\n",
        "\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, img_size=224, in_channels=3, num_classes=21843, loss_weights=None, loss_type = 'CrossEntropy', model_type = 'ResNet18', pretrained = False, use_clinical_data = False, fusion_strategy = 'learned_features', combination_type = 'concat', clin_feats = '10'):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.loss_type = loss_type\n",
        "        self.loss_weights = loss_weights\n",
        "        self.in_channels = in_channels\n",
        "        self.use_clinical_data = use_clinical_data\n",
        "        self.fusion_strategy = fusion_strategy\n",
        "        self.combination_type = combination_type\n",
        "        if self.loss_type == 'CrossEntropy':\n",
        "            self.loss_fct = CrossEntropyLoss(weight = self.loss_weights if torch.is_tensor(self.loss_weights) else None)\n",
        "            \n",
        "        \n",
        "        if model_type == 'ResNet18':\n",
        "            self.classifier = torchvision.models.resnet18(pretrained)\n",
        "        elif model_type == 'ResNet50':\n",
        "            self.classifier = torchvision.models.resnet50(pretrained)\n",
        "        elif model_type == 'ResNet101':\n",
        "            self.classifier = torchvision.models.resnet101(pretrained)\n",
        "        elif model_type == 'AlexNet':\n",
        "            self.classifier = torchvision.models.alexnet(pretrained)\n",
        "        elif model_type == 'DenseNet121':\n",
        "            self.classifier = torchvision.models.densenet121(pretrained)\n",
        "        elif model_type == 'Vgg16':\n",
        "            self.classifier = torchvision.models.vgg16(pretrained)\n",
        "        elif model_type == 'MobileNet_v2':\n",
        "            self.classifier = torchvision.models.mobilenet_v2(pretrained)\n",
        "        elif model_type == 'EfficientNet_b6':\n",
        "            self.classifier = torchvision.models.efficientnet_b6(pretrained)\n",
        "        elif model_type == 'EfficientNet_b5':\n",
        "            self.classifier = torchvision.models.efficientnet_b5(pretrained)\n",
        "        elif 'CoAtNet' in model_type:\n",
        "            if '0' in model_type:\n",
        "                num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "                channels = [64, 96, 192, 384, 768]      # D\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            elif '1' in model_type:\n",
        "                num_blocks = [2, 2, 6, 14, 2]\n",
        "                channels = [64, 96, 192, 384, 768]\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            elif '2' in model_type:\n",
        "                num_blocks = [2, 2, 6, 14, 2]\n",
        "                channels = [128, 128, 256, 512, 1026]\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            elif '3' in model_type:\n",
        "                num_blocks = [2, 2, 6, 14, 2]\n",
        "                channels = [192, 192, 384, 768, 1536]\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            elif '4' in model_type:\n",
        "                num_blocks = [2, 2, 12, 28, 2]\n",
        "                channels = [192, 192, 384, 768, 1536]\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            elif '5' in model_type: #changing DIM head from 32 to 64\n",
        "                num_blocks = [2, 2, 12, 28, 2]\n",
        "                channels = [192, 256, 512, 1280, 2048]\n",
        "                block_types = ['C', 'C', 'T', 'T']\n",
        "            self.classifier = CoAtNet((img_size, img_size), in_channels, num_blocks, channels, num_classes, block_types = block_types)\n",
        "            \n",
        "        #change the first conv layer according to the model_type\n",
        "        if in_channels != 3:\n",
        "            if 'ResNet' in model_type:\n",
        "                self.classifier.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = self.classifier.conv1.out_channels, kernel_size = self.classifier.conv1.kernel_size, stride = self.classifier.conv1.stride, padding = self.classifier.conv1.padding, bias = True if not self.classifier.conv1.bias is None else False)\n",
        "            elif model_type in ['AlexNet', 'Vgg16']:\n",
        "                self.classifier.features[0]= nn.Conv2d(in_channels = in_channels, out_channels = self.classifier.features[0].out_channels, kernel_size = self.classifier.features[0].kernel_size, stride = self.classifier.features[0].stride, padding = self.classifier.features[0].padding, bias = True if not self.classifier.features[0].bias is None else False)\n",
        "            elif 'DenseNet' in model_type:\n",
        "                self.classifier.features.conv0= nn.Conv2d(in_channels = in_channels, out_channels = self.classifier.features.conv0.out_channels, kernel_size = self.classifier.features.conv0.kernel_size, stride = self.classifier.features.conv0.stride, padding = self.classifier.features.conv0.padding, bias = True if not self.classifier.features.conv0.bias is None else False)\n",
        "            elif 'MobileNet' in model_type or 'EfficientNet' in model_type:\n",
        "                self.classifier.features[0][0]= nn.Conv2d(in_channels = in_channels, out_channels = self.classifier.features[0][0].out_channels, kernel_size = self.classifier.features[0][0].kernel_size, stride = self.classifier.features[0][0].stride, padding = self.classifier.features[0][0].padding, bias = True if not self.classifier.features[0][0].bias is None else False)\n",
        "            elif 'CoAtNet' in model_type:\n",
        "                self.classifier.s0[0][0] = nn.Conv2d(in_channels = in_channels, out_channels = self.classifier.s0[0][0].out_channels, kernel_size = self.classifier.s0[0][0].kernel_size, stride = self.classifier.s0[0][0].stride, padding = self.classifier.s0[0][0].padding, bias = True if not self.classifier.s0[0][0].bias is None else False)\n",
        "                \n",
        "        \n",
        "        fc = None\n",
        "        #define the last fully connected layer and define new layers if clinical data are used  \n",
        "        if self.use_clinical_data:\n",
        "            fc = torch.nn.Sequential(\n",
        "                torch.nn.Linear(2048, 512),\n",
        "                torch.nn.ReLU(inplace=True)\n",
        "            )                \n",
        "            \n",
        "            if self.fusion_strategy == 'learned_features':\n",
        "                self.clinical_feats =  torch.nn.Sequential(\n",
        "                    nn.Linear(clin_feats, 1024),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Linear(1024, 512),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "            \n",
        "            self.final_classifier = torch.nn.Sequential(\n",
        "                torch.nn.Dropout(p=0.25),\n",
        "                torch.nn.Linear(512*2 if self.mode == \"concat\" else 512, 512),\n",
        "                torch.nn.ReLU(inplace=True),\n",
        "                torch.nn.Dropout(p=0.25),\n",
        "                torch.nn.Linear(512, 1)\n",
        "            )\n",
        "        #change the last selected fc layer according to the model_type\n",
        "        if 'ResNet' in model_type:\n",
        "            if fc is None:\n",
        "                fc = nn.Linear(in_features = self.classifier.fc.in_features, out_features=num_classes)\n",
        "            if self.classifier.fc.bias is None:\n",
        "                fc.bias = None\n",
        "            self.classifier.fc = fc\n",
        "        elif model_type in ['AlexNet', 'Vgg16']:\n",
        "            if fc is None:\n",
        "                fc = nn.Linear(in_features = self.classifier.classifier[6].in_features, out_features=num_classes)\n",
        "            if self.classifier.classifier[6].bias is None:\n",
        "                fc.bias = None\n",
        "            self.classifier.classifier[6]= fc\n",
        "        elif 'DenseNet' in model_type:\n",
        "            if fc is None:\n",
        "                fc = nn.Linear(in_features = self.classifier.classifier.in_features, out_features=num_classes)\n",
        "            if self.classifier.classifier.bias is None:\n",
        "                fc.bias = None\n",
        "            self.classifier.classifier= fc\n",
        "        elif 'MobileNet' in model_type or 'EfficientNet' in model_type:\n",
        "            if fc is None:\n",
        "                fc = nn.Linear(in_features = self.classifier.classifier[1].in_features, out_features=num_classes)\n",
        "            if self.classifier.classifier[1].bias is None:\n",
        "                fc.bias = None\n",
        "            self.classifier.classifier[1] = fc\n",
        "        elif 'CoAtNet' in model_type:\n",
        "            fc = nn.Linear(in_features = self.classifier.fc.in_features, out_features=num_classes)\n",
        "            if self.classifier.fc.bias is None:\n",
        "                fc.bias = None\n",
        "            self.classifier.fc = fc\n",
        "        \n",
        "    def forward(self, x, labels = None, clinical_feats = None):\n",
        "        logits = self.classifier(x)\n",
        "        \n",
        "        if self.use_clinical_data:\n",
        "            if self.fusion_strategy == 'learned_features':\n",
        "                cl = self.clinical_feats(clinical_feats)\n",
        "            elif self.fusion_strategy == 'features':\n",
        "                cl = clinical_feats\n",
        "            \n",
        "            if self.combination_type == 'concat':\n",
        "                x = torch.cat((logits,cl), dim = -1)\n",
        "            elif self.combination_type == 'sum':\n",
        "                x = logits + cl\n",
        "            elif self.combination_type == 'mul':\n",
        "                x = logits * cl\n",
        "            \n",
        "            logits = self.final_classifier(x)\n",
        "        \n",
        "        if labels is not None:\n",
        "            '''\n",
        "            if self.loss_type == 'CrossEntropy':\n",
        "                self.loss_fct = CrossEntropyLoss(weight = self.loss_weights.to(x.device) if torch.is_tensor(self.loss_weights) else None)\n",
        "                '''                \n",
        "            loss = self.loss_fct(logits.view(-1, self.num_classes), labels)\n",
        "            return logits, loss\n",
        "        \n",
        "        return logits, None\n",
        "        \n",
        "\n",
        "CONFIGS = {\n",
        "    'ViT-B_16': configs.get_b16_config(),\n",
        "    'ViT-B_32': configs.get_b32_config(),\n",
        "    'ViT-L_16': configs.get_l16_config(),\n",
        "    'ViT-L_32': configs.get_l32_config(),\n",
        "    'ViT-H_14': configs.get_h14_config(),\n",
        "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
        "    'R50-ViT-MRI' : configs.get_r50_MRI_config(),\n",
        "    'ViT-MRI' : configs.get_MRI_config(),\n",
        "    'ViT-1Lay' : configs.get_MinConfig(),\n",
        "    'testing': configs.get_testing(),\n",
        "    'ViT-half':configs.get_halfHidden(),\n",
        "    'ViT-half2':configs.get_halfHiddenHeads(),\n",
        "    'ViT-h12l2':configs.get_h12l2Config(),\n",
        "    'ViT-h8l2': configs.get_h8l2Config(),\n",
        "    'ViT-h8l2hid384' : configs.get_h8l2hid384Config(),\n",
        "    'ViT-h4l2hid384' : configs.get_h4l2hid384Config(),\n",
        "    'ViT-parallel_h12l2': configs.get_ParallelConfig_h12l2(),\n",
        "    'ViT-parallel': configs.get_ParallelViT_config(),}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQhmLCikgwui"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "import platform\n",
        "from monai.data import CacheDataset\n",
        "\n",
        "class NGSLungDataset(CacheDataset):\n",
        "    def __init__(self, root_dir, split_path, section, num_fold, transforms, seed = 100, cache_num = sys.maxsize, cache_rate=1.0, num_workers=0, execute_test = True):    \n",
        "        #if execute test is False, training and test split are used both for traning. \n",
        "  \n",
        "        if not os.path.isdir(root_dir):\n",
        "            raise ValueError(\"Root directory root_dir must be a directory.\")\n",
        "        self.section = section\n",
        "        self.text_labels = ['negative', 'positive']\n",
        "        #self.transforms = transforms\n",
        "        self.num_fold = num_fold\n",
        "        self.seed = seed\n",
        "        self.execute_test = execute_test\n",
        "        \n",
        "        data = self._generate_data_list(split_path)\n",
        "        super().__init__(data, transforms, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
        "        \n",
        "     \n",
        "    #split data in train, val and test sets in a reproducible way\n",
        "    def _generate_data_list(self, split_path):\n",
        "        with open(split_path) as fp:\n",
        "           path=json.load(fp)\n",
        "        data = list()\n",
        "\n",
        "        \n",
        "        if self.section == 'test':\n",
        "            data = path[f'fold{self.num_fold}']['test'] if self.execute_test else []\n",
        "        elif self.section == 'training':\n",
        "            data = path[f'fold{self.num_fold}']['train']\n",
        "            if not self.execute_test:\n",
        "                data = data + path[f'fold{self.num_fold}']['test']\n",
        "        elif self.section == 'validation':\n",
        "            data = path[f'fold{self.num_fold}']['val']\n",
        "        else: \n",
        "            raise ValueError(\n",
        "                    f\"Unsupported section: {self.section}, \"\n",
        "                    \"available options are ['training', 'validation', 'test'].\"\n",
        "                )\n",
        "        \n",
        "        if platform.system() != 'Windows':\n",
        "            for sample in data:\n",
        "                for key in sample.keys():\n",
        "                    if isinstance(sample[key], str):\n",
        "                        sample[key] = sample[key].replace('\\\\', '/')\n",
        "        return data     \n",
        "    '''\n",
        "    def get_label_proportions(self):\n",
        "        c = [None]*2\n",
        "        label_props = [None]*2\n",
        "        for i in range(2):\n",
        "            c[i] = len([el['label'] for el in self.data if el['label'] == i])\n",
        "        for i in range(len(c)):\n",
        "            label_props[i] = max(c)/c[i]\n",
        "        return label_props\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybVkyLecYFX"
      },
      "source": [
        "# Trying to solve the ITK reader problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o7J9kN_maEs",
        "outputId": "52f06fc0-d56c-4570-d177-198e1cf4babc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MONAI' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/ViT_Lung_Cancer-main/MONAI\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/ViT_Lung_Cancer-main/MONAI\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from monai==0+unknown) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from monai==0+unknown) (1.21.6)\n",
            "Collecting itk>=5.2\n",
            "  Downloading itk-5.2.1.post1-cp37-cp37m-manylinux2014_x86_64.whl (8.3 kB)\n",
            "Collecting itk-registration==5.2.1.post1\n",
            "  Downloading itk_registration-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.3 MB)\n",
            "\u001b[K     || 20.3 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting itk-numerics==5.2.1.post1\n",
            "  Downloading itk_numerics-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.5 MB)\n",
            "\u001b[K     || 54.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting itk-segmentation==5.2.1.post1\n",
            "  Downloading itk_segmentation-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
            "\u001b[K     || 16.6 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting itk-filtering==5.2.1.post1\n",
            "  Downloading itk_filtering-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (95.3 MB)\n",
            "\u001b[K     || 95.3 MB 37 kB/s \n",
            "\u001b[?25hCollecting itk-core==5.2.1.post1\n",
            "  Downloading itk_core-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (70.6 MB)\n",
            "\u001b[K     || 70.6 MB 8.8 kB/s \n",
            "\u001b[?25hCollecting itk-io==5.2.1.post1\n",
            "  Downloading itk_io-5.2.1.post1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[K     || 15.0 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->monai==0+unknown) (4.1.1)\n",
            "Installing collected packages: itk-core, itk-numerics, itk-filtering, itk-segmentation, itk-registration, itk-io, monai, itk\n",
            "  Attempting uninstall: monai\n",
            "    Found existing installation: monai 0.9.0\n",
            "    Uninstalling monai-0.9.0:\n",
            "      Successfully uninstalled monai-0.9.0\n",
            "  Running setup.py develop for monai\n",
            "Successfully installed itk-5.2.1.post1 itk-core-5.2.1.post1 itk-filtering-5.2.1.post1 itk-io-5.2.1.post1 itk-numerics-5.2.1.post1 itk-registration-5.2.1.post1 itk-segmentation-5.2.1.post1 monai\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: itk in /usr/local/lib/python3.7/dist-packages (5.2.1.post1)\n",
            "Requirement already satisfied: itk-filtering==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-numerics==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-segmentation==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from itk) (1.21.6)\n",
            "Requirement already satisfied: itk-io==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-core==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-registration==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk) (5.2.1.post1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai[itk]\n",
            "  Using cached monai-0.9.0-202206131636-py3-none-any.whl (939 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (1.11.0+cu113)\n",
            "Requirement already satisfied: itk>=5.2 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-numerics==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-segmentation==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-registration==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-io==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-filtering==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-core==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->monai[itk]) (4.1.1)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "monai"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     || 48.4 MB 21 kB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: monai[itk] in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (1.21.6)\n",
            "Requirement already satisfied: itk>=5.2 in /usr/local/lib/python3.7/dist-packages (from monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-filtering==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-numerics==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-registration==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-io==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-segmentation==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: itk-core==5.2.1.post1 in /usr/local/lib/python3.7/dist-packages (from itk>=5.2->monai[itk]) (5.2.1.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->monai[itk]) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  !git clone https://github.com/Project-MONAI/MONAI.git\n",
        "  %cd MONAI\n",
        "  !pip install -e '.[itk]'\n",
        "except:\n",
        "  pass\n",
        "!pip install itk\n",
        "#!pip install itk==5.3rc4 # trying a different version\n",
        "!pip install 'monai[itk]'\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/ViT_Lung_Cancer-main\")\n",
        "\n",
        "!python -c \"import monai\" || pip install -q \"monai-weekly[itk, pillow]\"\n",
        "!pip install -q \"SimpleITK\"\n",
        "!pip install 'monai[itk]'\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "from monai.data import ITKReader, PILReader\n",
        "from monai.transforms import (\n",
        "    LoadImage, LoadImaged, EnsureChannelFirstd,\n",
        "    Resized, EnsureTyped, Compose)\n",
        "from monai.config import print_config\n",
        "import SimpleITK as sitk\n",
        "#!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE7QeukhBOZ9"
      },
      "source": [
        "# get_SpecificLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xjn4MvJ4maQ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import math\n",
        "\n",
        "from monai.data.image_reader import ITKReader\n",
        "\n",
        "# from dataset.NGSLungDatasetCV import NGSLungDataset as DS\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from utils.transforms import (CorrectSpacing, \n",
        "                              PatchedImage, \n",
        "                              ResizeWithRatioD,\n",
        "                              CenterPatchedImage,\n",
        "                              ListPatchedImage,\n",
        "                              ResizeWithRatioDVariableDim, \n",
        "                              PatchedImageAllSlice,\n",
        "                              TensorPad, ConvertListToTensor, \n",
        "                              MyCropForegroundd, \n",
        "                              ExpandDims, \n",
        "                              ConcatChDim, \n",
        "                              AsDepthFirstD, \n",
        "                              DeleteKeys,\n",
        "                              PrepareClinicalData, \n",
        "                              DeleteNotUsableClinicalData)\n",
        "from monai.transforms import (DivisiblePadD,  \n",
        "    LoadImageD, \n",
        "    NormalizeIntensityD,\n",
        "    AddChannelD,\n",
        "    Compose,\n",
        "    RandFlipD,\n",
        "    RandRotate90D,\n",
        "    ToTensorD,\n",
        "    ScaleIntensityD,\n",
        "    SpacingD,\n",
        "    OrientationD,\n",
        "    RandCropByPosNegLabelD,\n",
        "    RandSpatialCropD,\n",
        "    CropForegroundD, \n",
        "    ScaleIntensityRanged, \n",
        "    IdentityD)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_loss_weights(split_path, label_key):\n",
        "    path_data = None\n",
        "    if '114sample' in split_path:\n",
        "        path_data = os.path.join('data','ngslung_pathAllGene114.json')\n",
        "    elif '131samples' in split_path:\n",
        "        path_data = os.path.join('data','ngslung_pathAllGene131_WithCombo_MultiLabel.json')\n",
        "    \n",
        "    if path_data is not None:\n",
        "        with open(path_data) as fp:\n",
        "            d = json.load(fp)\n",
        "            lbl = [el[label_key] for el in d]\n",
        "            num = len(lbl)\n",
        "            pos = sum(lbl)\n",
        "            neg = num - pos\n",
        "            p_pos = num/pos\n",
        "            p_neg = num/neg\n",
        "            weights = [p_neg/max(p_pos, p_neg), p_pos/max(p_pos, p_neg)]\n",
        "        return torch.tensor(weights)\n",
        "    return None\n",
        "\n",
        "def get_SpecificLoader(dataset, label_key, img_size, num_patches, section, train_batch_size, root_dir, split_path, num_fold, execute_test, inner_loop_idx = None, eval_stride=1, window = 'parenchyma', k_divisible=74, padding=True):\n",
        "    KEYS = ('image','mask', label_key)\n",
        "    \n",
        "    patch_per_side = int(math.sqrt(num_patches))\n",
        "    spatial_size = (-1, int(img_size/patch_per_side), int(img_size/patch_per_side))\n",
        "    \n",
        "    if dataset in ['NGSLUNG']:\n",
        "        train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            SpacingD(keys = KEYS[:-1], pixdim=(1., 1., 1.), mode = (\"bilinear\")),\n",
        "            NormalizeIntensityD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "            DivisiblePadD(KEYS[0], k = spatial_size, mode = 'constant'),\n",
        "            RandCropByPosNegLabelD(keys = KEYS[:-1], label_key=KEYS[1], spatial_size = (spatial_size[1],spatial_size[2],num_patches), pos = 1, neg = 0),\n",
        "            RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),    \n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "            ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            SpacingD(keys = KEYS[:-1], pixdim=(1., 1., 1.), mode = (\"bilinear\")),\n",
        "            NormalizeIntensityD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "            DivisiblePadD(KEYS[0], k = spatial_size[1], mode = 'constant'),\n",
        "            RandCropByPosNegLabelD(keys = KEYS[:-1], label_key=KEYS[1], spatial_size = (spatial_size[1],spatial_size[2],num_patches), pos = 1, neg = 0),\n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "            ])\n",
        "    elif dataset in ['NGSLUNG_crop']:\n",
        "        train_transforms = Compose([\n",
        "        LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            SpacingD(keys = KEYS[:-1], pixdim=(1., 1., 1.), mode = (\"bilinear\")),\n",
        "            NormalizeIntensityD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "            DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = 'constant'),\n",
        "            RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),    \n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "        ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                #TransposeITKD(keys = KEYS[:-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                SpacingD(keys = KEYS[:-1], pixdim=(1., 1., 1.), mode = (\"bilinear\")),\n",
        "                NormalizeIntensityD(keys = KEYS[:-1]),\n",
        "                ScaleIntensityD(keys = KEYS[:-1]),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = 'constant'),\n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                #PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "                CenterPatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "            ])\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_crop_ValidVoting']:\n",
        "        train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "            DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = 'constant'),\n",
        "            RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),    \n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "        ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "                LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                #TransposeITKD(keys = KEYS[:-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                ScaleIntensityD(keys = KEYS[:-1]),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = 'constant'),\n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                ListPatchedImage(KEYS[:-1], num_patches = num_patches, stride = eval_stride)\n",
        "            ])\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_crop_ValidVoting_ScaledRange']:\n",
        "        print('dataset: ', dataset)\n",
        "        if window == 'parenchyma':\n",
        "            a_min = -1350\n",
        "            a_max = 150\n",
        "        elif window == 'mediastinum':\n",
        "            a_min = -115\n",
        "            a_max = 235\n",
        "        else:\n",
        "            raise ValueError('Error! Invalid window name.')\n",
        "        print(spatial_size[1])\n",
        "        k_divisible = [k_divisible,k_divisible, 1]\n",
        "          \n",
        "        \n",
        "        if 'multi_label' in label_key:\n",
        "            train_transforms = Compose([\n",
        "                LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                ConvertListToTensor(keys = KEYS[-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                MyCropForegroundd(KEYS[:-1], KEYS[1], k_divisible = k_divisible),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = padding),\n",
        "                RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "                RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),    \n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "            ])\n",
        "            \n",
        "            val_transforms = Compose([\n",
        "                LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                ConvertListToTensor(keys = KEYS[-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                MyCropForegroundd(KEYS[:-1], KEYS[1], k_divisible = k_divisible),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[:-1], k = (spatial_size[1],spatial_size[2],-1), mode = padding),\n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                ListPatchedImage(KEYS[:-1], num_patches = num_patches, stride = eval_stride)\n",
        "                ])\n",
        "        else:    \n",
        "            train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                #TransposeITKD(keys = KEYS[:-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                MyCropForegroundd(KEYS[:-1], KEYS[1], k_divisible = k_divisible),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[0], k = (spatial_size[1],spatial_size[2],-1), mode = padding),\n",
        "                RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "                RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),    \n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                PatchedImage(KEYS[:-1], num_patches = num_patches)\n",
        "            ])\n",
        "            \n",
        "            val_transforms = Compose([\n",
        "                LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "                #TransposeITKD(keys = KEYS[:-1]),\n",
        "                AddChannelD(keys = KEYS[:-1]),\n",
        "                CorrectSpacing(KEYS[:-1]),\n",
        "                ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "                OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "                MyCropForegroundd(KEYS[:-1], KEYS[1], k_divisible = k_divisible),\n",
        "                ResizeWithRatioD(keys = KEYS[:-1], image_size=spatial_size[1]),\n",
        "                DivisiblePadD(KEYS[:-1], k = (spatial_size[1],spatial_size[2],-1), mode = padding),\n",
        "                ToTensorD(keys = KEYS[:-1]),\n",
        "                ListPatchedImage(KEYS[:-1], num_patches = num_patches, stride = eval_stride)\n",
        "                ])\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_crop_ScaledRange_AllVolume']:\n",
        "        #create a 2D patched image using all patch of the volume\n",
        "        if window == 'parenchyma':\n",
        "            a_min = -1350\n",
        "            a_max = 150\n",
        "        elif window == 'mediastinum':\n",
        "            a_min = -115\n",
        "            a_max = 235\n",
        "        else:\n",
        "            raise ValueError('Error! Invalid window name.')\n",
        "            \n",
        "        train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioDVariableDim(keys = KEYS[0], image_size=img_size),\n",
        "            RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),\n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImageAllSlice(KEYS[:-1]),\n",
        "            TensorPad(KEYS[0], image_size = img_size, pad_value = 0),\n",
        "        ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            #TransposeITKD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            ScaleIntensityRanged(keys = KEYS[0], a_min = a_min, a_max = a_max, b_min = 0, b_max=1, clip = True),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioDVariableDim(keys = KEYS[0], image_size=img_size),\n",
        "            ToTensorD(keys = KEYS[:-1]),\n",
        "            PatchedImageAllSlice(KEYS[:-1]),\n",
        "            TensorPad(KEYS[0], image_size = img_size, pad_value = 0),\n",
        "        ])\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_crop_ScaledRange_3D']:\n",
        "        \n",
        "        pad_depth = math.ceil(max_depth/depth_kernel_size)*depth_kernel_size\n",
        "        \n",
        "        if window == 'parenchyma':\n",
        "            a_min = -1350\n",
        "            a_max = 150\n",
        "        elif window == 'mediastinum':\n",
        "            a_min = -115\n",
        "            a_max = 235\n",
        "        else:\n",
        "            raise ValueError('Error! Invalid window name.')\n",
        "        \n",
        "        train_trans_list = [\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            ScaleIntensityRanged(keys = KEYS[0], a_min = -1350, a_max = 150, b_min = 0, b_max=1, clip = True),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=img_size),\n",
        "            DivisiblePadD(keys = KEYS[:-1], k = (img_size,img_size,-1), method = 'symmetric')]\n",
        "        \n",
        "        if train_batch_size != 1 : \n",
        "            train_trans_list = train_trans_list + [ DivisiblePadD(keys = KEYS[:-1], k = (-1,-1,pad_depth), method = 'end')\n",
        "                ]\n",
        "        \n",
        "        train_trans_list = train_trans_list + [ RandFlipD(keys = KEYS[:-1], prob = 0.5, spatial_axis=0),\n",
        "                                                RandRotate90D(keys = KEYS[:-1], prob=0.5, spatial_axes=(0,1)),\n",
        "                                                ToTensorD(keys = KEYS[:-1])\n",
        "                                                ]\n",
        "        \n",
        "        train_transforms = Compose(train_trans_list)\n",
        "        \n",
        "        val_trans_list = [\n",
        "            LoadImageD(keys = KEYS[:-1], reader = 'ITKReader'),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            CorrectSpacing(KEYS[:-1]),\n",
        "            ScaleIntensityRanged(keys = KEYS[0], a_min = -1350, a_max = 150, b_min = 0, b_max=1, clip = True),\n",
        "            OrientationD(keys = KEYS[:-1], axcodes = 'RAS'),\n",
        "            CropForegroundD(KEYS[:-1], KEYS[1]),\n",
        "            ResizeWithRatioD(keys = KEYS[:-1], image_size=img_size),\n",
        "            DivisiblePadD(keys = KEYS[:-1], k = (img_size,img_size,-1), method = 'symmetric')]\n",
        "        \n",
        "        if train_batch_size != 1 : \n",
        "            val_trans_list = val_trans_list + [ DivisiblePadD(keys = KEYS[:-1], k = (-1,-1,pad_depth), method = 'end')\n",
        "                ]\n",
        "        \n",
        "        val_trans_list = val_trans_list + [ ToTensorD(keys = KEYS[:-1])]\n",
        "        \n",
        "        val_transforms = Compose(val_trans_list)\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_attMap']:\n",
        "        KEYS = ('patched_img','att_maps', label_key)\n",
        "        train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            ExpandDims(keys = KEYS[:-1]),\n",
        "            RandSpatialCropD(keys = KEYS[:-1],roi_size=[-1,-1,1], random_size=False),\n",
        "            ConcatChDim(keys = KEYS[:-1], deleteLastDimIf1 = True),\n",
        "            RandFlipD(keys = KEYS[0], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[0], prob=0.5, spatial_axes=(0,1)),    \n",
        "            ToTensorD(keys = KEYS[0])\n",
        "            ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            ExpandDims(keys = KEYS[:-1]),\n",
        "            ConcatChDim(keys = KEYS[:-1]),\n",
        "            AsDepthFirstD(keys = KEYS[0]),\n",
        "            ToTensorD(keys = KEYS[0])\n",
        "            ])\n",
        "        \n",
        "    elif dataset in ['NGSLUNG_patchedImage']:\n",
        "        KEYS = ('patched_img', label_key)\n",
        "        del_keys = ['image','mask','att_maps_meta_dict','patched_img_meta_dict']    \n",
        "        train_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            ExpandDims(keys = KEYS[:-1]),\n",
        "            RandSpatialCropD(keys = KEYS[:-1],roi_size=[-1,-1,1], random_size=False),\n",
        "            DeleteKeys(keys = KEYS[:-1], del_keys = del_keys,deleteLastDimIf1 = True),\n",
        "            RandFlipD(keys = KEYS[0], prob = 0.5, spatial_axis=0),\n",
        "            RandRotate90D(keys = KEYS[0], prob=0.5, spatial_axes=(0,1)),    \n",
        "            ToTensorD(keys = KEYS[0])\n",
        "            ])\n",
        "        \n",
        "        val_transforms = Compose([\n",
        "            LoadImageD(keys = KEYS[:-1]),\n",
        "            AddChannelD(keys = KEYS[:-1]),\n",
        "            ScaleIntensityD(keys = KEYS[:-1]),\n",
        "            ExpandDims(keys = KEYS[:-1]),\n",
        "            DeleteKeys(keys = KEYS[:-1],del_keys = del_keys,),\n",
        "            AsDepthFirstD(keys = KEYS[0]),\n",
        "            ToTensorD(keys = KEYS[0])\n",
        "            ])\n",
        "    \n",
        "    if section == 'training':\n",
        "        dataset = NGSLungDataset(root_dir = root_dir, split_path = split_path, section = 'training', num_fold = num_fold, transforms = train_transforms, execute_test = execute_test)\n",
        "        shuffle = True\n",
        "    elif section in ['validation', 'test']:\n",
        "        dataset = NGSLungDataset(root_dir = root_dir, split_path = split_path, section = section, num_fold = num_fold, transforms = val_transforms, execute_test = execute_test)\n",
        "        shuffle = False\n",
        "    \n",
        "    print(f'Section: {section}, Shuffle: {shuffle}')\n",
        "    \n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size = train_batch_size,\n",
        "                        num_workers=0,\n",
        "                        shuffle = shuffle \n",
        "                        )\n",
        "    \n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRS8PwpyGsfr"
      },
      "source": [
        "# New model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEOMIeAkBAO3"
      },
      "outputs": [],
      "source": [
        "class ViTResNet(nn.Module):\n",
        "    def __init__(self, config, img_size=224, in_channels=3, num_classes=21843, loss_weights=None, zero_head=True, vis=True, embeddings_type = '2D', loss_type = 'CrossEntropy', classifier_net = 'ResNet18', vit_pretrained_dir = None):\n",
        "        super(ViTResNet, self).__init__()\n",
        "        self.vit = VisionTransformer(config=config, img_size=img_size, in_channels=in_channels, num_classes=num_classes, loss_weights=loss_weights, zero_head=zero_head, vis=vis, embeddings_type = embeddings_type, loss_type = loss_type)\n",
        "        self.classifier = CNNClassifier(img_size=img_size, in_channels=in_channels, num_classes=num_classes, loss_type=loss_type, model_type = classifier_net)\n",
        "        if vit_pretrained_dir is not None:\n",
        "            self.vit.load_state_dict(torch.load(vit_pretrained_dir))\n",
        "\n",
        "    def forward(self, x, labels = None):\n",
        "        print(len(x))\n",
        "        imgs = x.copy()\n",
        "        x, attn_weights = self.vit(x)\n",
        "        att_map = compute_att_map(attn_weights, x.device, imgs.shape)\n",
        "        #print(att_map.shape)\n",
        "        x, _ = self.classifier(np.concatenate(imgs,x))\n",
        "        return x, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbDjvanxHAj7"
      },
      "source": [
        "## Training the new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GZEAuRm5SuSa",
        "outputId": "167095fb-5d5a-490c-e1c9-787793bc19ee"
      },
      "outputs": [],
      "source": [
        "loader = get_SpecificLoader(dataset=\"NGSLUNG_crop_ValidVoting_ScaledRange\", label_key=\"EGFR\", img_size=224, num_patches=9, section=\"training\", train_batch_size=32, root_dir=\"data\", split_path=\"data/5BalancedCrossValFold_EGFR_131samples.json\", num_fold=4, execute_test=True, inner_loop_idx = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRg_KyJfHAOX"
      },
      "outputs": [],
      "source": [
        "model = ViTResNet(CONFIGS['ViT-h12l2'], num_classes=2)\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer.zero_grad() # io\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        # get the inputs; data is (should be) a list of [inputs, labels]\n",
        "        inputs, labels = loader[i]\n",
        "        # print(data)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ViT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "c27bedd56da27e739c543acd41cc9031fe9b04d3366ad916baa73e602483ec56"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
